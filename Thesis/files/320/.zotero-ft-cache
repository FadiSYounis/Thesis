
Cornell University
Cornell University Library
We gratefully acknowledge support from
the Simons Foundation
and member institutions
arXiv.org > cs > arXiv:1712.07107
( Help | Advanced search )
Full-text links:
Download:

    PDF
    Other formats

( license )
Current browse context:
cs.LG
< prev  |  next >
new  | recent  | 1712
Change to browse by:
cs
cs.CR
cs.CV
stat
stat.ML
References & Citations

    NASA ADS

Bookmark
( what is this? )
CiteULike logo BibSonomy logo Mendeley logo del.icio.us logo Digg logo Reddit logo ScienceWISE logo
Computer Science > Learning
Title: Adversarial Examples: Attacks and Defenses for Deep Learning
Authors: Xiaoyong Yuan , Pan He , Qile Zhu , Rajendra Rana Bhat , Xiaolin Li
(Submitted on 19 Dec 2017 ( v1 ), last revised 5 Jan 2018 (this version, v2))

    Abstract: With rapid progress and great successes in a wide spectrum of applications, deep learning is being applied in many safety-critical environments. However, deep neural networks have been recently found vulnerable to well-designed input samples, called \textit{adversarial examples}. Adversarial examples are imperceptible to human but can easily fool deep neural networks in the testing/deploying stage. The vulnerability to adversarial examples becomes one of the major risks for applying deep neural networks in safety-critical scenarios. Therefore, the attacks and defenses on adversarial examples draw great attention.
    In this paper, we review recent findings on adversarial examples against deep neural networks, summarize the methods for generating adversarial examples, and propose a taxonomy of these methods. Under the taxonomy, applications and countermeasures for adversarial examples are investigated. We further elaborate on adversarial examples and explore the challenges and the potential solutions. 

Comments: 	21 pages, 13 figures, Github: this https URL
Subjects: 	Learning (cs.LG) ; Cryptography and Security (cs.CR); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (stat.ML)
Cite as: 	arXiv:1712.07107 [cs.LG]
  	(or arXiv:1712.07107v2 [cs.LG] for this version)
Submission history
From: Xiaoyong Yuan [ view email ]
[v1] Tue, 19 Dec 2017 18:44:07 GMT (8019kb,D)
[v2] Fri, 5 Jan 2018 15:51:54 GMT (8021kb,D)
Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )

Link back to: arXiv , form interface , contact .
Twitter
